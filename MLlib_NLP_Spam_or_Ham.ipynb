{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a spam filter using various NLP tools and Naive Bayes classifier.  \n",
    "The dataset for Spam Detection is from: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and get spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import (Tokenizer,\n",
    "                                StopWordsRemover,\n",
    "                                CountVectorizer,\n",
    "                                IDF,\n",
    "                                StringIndexer,\n",
    "                                VectorAssembler)\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('NLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('smsspamcollection/SMSSpamCollection', sep='\\t', inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('_c0', 'class').withColumnRenamed('_c1', 'text')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new feature column: count lenght of each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('length', length(df['text']))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check mean length of text for each class (spam or ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('class').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data to numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam_to_num = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer  \n",
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). A simple Tokenizer class provides this functionality.\n",
    "RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence to a list of words\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal  \n",
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning.\n",
    "\n",
    "StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter. Default stop words for some languages are accessible by calling StopWordsRemover.loadDefaultStopWords(language), for which available options are “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”. A boolean parameter caseSensitive indicates if the matches should be case sensitive (false by default).  \n",
    "```\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)\n",
    "\n",
    "+---+----------------------------+--------------------+\n",
    "|id |raw                         |filtered            |\n",
    "+---+----------------------------+--------------------+\n",
    "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
    "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
    "+---+----------------------------+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopremove = StopWordsRemover(inputCol='token_text', outputCol='stop_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer  \n",
    "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
    "\n",
    "During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts.  \n",
    "```\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"d b c a d\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "print('original df')\n",
    "df.show()\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", \n",
    "                     vocabSize=4, # maximum unique words in the corpus\n",
    "                     minDF=1.0)   # Ex. minDF = 2 means \"ignore terms that appear in less than 2 documents\". \n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "print('df with count vectorizer')\n",
    "result.show(truncate=False)\n",
    "\n",
    "original df\n",
    "+---+---------------+\n",
    "| id|          words|\n",
    "+---+---------------+\n",
    "|  0|[d, b, c, a, d]|\n",
    "|  1|[a, b, b, c, a]|\n",
    "+---+---------------+\n",
    "\n",
    "df with count vectorizer\n",
    "+---+---------------+-------------------------------+\n",
    "|id |words          |features                       |\n",
    "+---+---------------+-------------------------------+\n",
    "|0  |[d, b, c, a, d]|(4,[0,1,2,3],[1.0,1.0,2.0,1.0])|\n",
    "|1  |[a, b, b, c, a]|(4,[0,1,3],[2.0,2.0,1.0])      |\n",
    "+---+---------------+-------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(inputCol='stop_tokens', outputCol='c_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"tf-idf\">TF-IDF</h3>\n",
    "\n",
    "<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Term frequency-inverse document frequency (TF-IDF)</a> \n",
    "is a feature vectorization method widely used in text mining to reflect the importance of a term \n",
    "to a document in the corpus. Denote a term by <code>$t$</code>, a document by  d , and the corpus by D.\n",
    "Term frequency <code>$TF(t, d)$</code> is the number of times that term <code>$t$</code> appears in document <code>$d$</code>, while \n",
    "document frequency <code>$DF(t, D)$</code> is the number of documents that contains term <code>$t$</code>. If we only use \n",
    "term frequency to measure the importance, it is very easy to over-emphasize terms that appear very \n",
    "often but carry little information about the document, e.g. &#8220;a&#8221;, &#8220;the&#8221;, and &#8220;of&#8221;. If a term appears \n",
    "very often across the corpus, it means it doesn&#8217;t carry special information about a particular document.\n",
    "Inverse document frequency is a numerical measure of how much information a term provides:\n",
    "\n",
    "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n",
    "\n",
    "where |D| is the total number of documents in the corpus. Since logarithm is used, if a term \n",
    "appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \n",
    "dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
    "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "Example\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"d b c a d\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "print('original df')\n",
    "df.show()\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"cv_features\", \n",
    "                     vocabSize=4, # maximum unique words in the corpus\n",
    "                     minDF=1.0)   # Ex. minDF = 2 means \"ignore terms that appear in less than 2 documents\". \n",
    "model = cv.fit(df)\n",
    "result = model.transform(df)\n",
    "print('df with count vectorizer')\n",
    "result.show(truncate=False)\n",
    "\n",
    "idf = IDF(inputCol=\"cv_features\", outputCol=\"idf_features\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "print('df with IDF')\n",
    "rescaledData.show(truncate=False)\n",
    "\n",
    "original df\n",
    "+---+---------------+\n",
    "| id|          words|\n",
    "+---+---------------+\n",
    "|  0|[d, b, c, a, d]|\n",
    "|  1|[a, b, b, c, a]|\n",
    "+---+---------------+\n",
    "\n",
    "df with count vectorizer\n",
    "+---+---------------+-------------------------------+\n",
    "|id |words          |cv_features                    |\n",
    "+---+---------------+-------------------------------+\n",
    "|0  |[d, b, c, a, d]|(4,[0,1,2,3],[1.0,1.0,2.0,1.0])|\n",
    "|1  |[a, b, b, c, a]|(4,[0,1,3],[2.0,2.0,1.0])      |\n",
    "+---+---------------+-------------------------------+\n",
    "\n",
    "df with IDF\n",
    "+---+---------------+-------------------------------+----------------------------------------------+\n",
    "|id |words          |cv_features                    |idf_features                                  |\n",
    "+---+---------------+-------------------------------+----------------------------------------------+\n",
    "|0  |[d, b, c, a, d]|(4,[0,1,2,3],[1.0,1.0,2.0,1.0])|(4,[0,1,2,3],[0.0,0.0,0.8109302162163288,0.0])|\n",
    "|1  |[a, b, b, c, a]|(4,[0,1,3],[2.0,2.0,1.0])      |(4,[0,1,3],[0.0,0.0,0.0])                     |\n",
    "+---+---------------+-------------------------------+----------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make features columns to run machine learning on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Model\n",
    "\n",
    "We'll use Naive Bayes classifier (Can try other classification models such as RF, Decison Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol='features',\n",
    "                labelCol='label',\n",
    "                predictionCol='prediction',\n",
    "                probabilityCol='probability') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num, \n",
    "                                  tokenizer, \n",
    "                                  stopremove,\n",
    "                                  count_vec,\n",
    "                                  idf,\n",
    "                                  clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.5 ms, sys: 2.74 ms, total: 45.3 ms\n",
      "Wall time: 7.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaner = data_prep_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|         stop_tokens|               c_vec|              tf_idf|            features|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
      "|  ham|Ok lar... Joking ...|    29|  0.0|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|(13424,[0,24,297,...|\n",
      "| spam|Free entry in 2 a...|   155|  1.0|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|(13424,[2,13,19,3...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = cleaner.transform(df)\n",
    "clean_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only 2 columns for ML in Spark\n",
    "clean_data = clean_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "(training, testing) = clean_data.randomSplit([0.8, 0.2])  # 80% and 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[-606.35872207312...|[1.0,1.4606590065...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[-445.26743833842...|[1.0,2.4606440248...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,18...|[-1368.9459678229...|[1.0,9.4011337917...|       0.0|\n",
      "|  0.0|(13424,[0,1,15,20...|[-669.20818569207...|[1.0,3.9110883342...|       0.0|\n",
      "|  0.0|(13424,[0,1,20,27...|[-972.56817871929...|[1.0,5.6114360786...|       0.0|\n",
      "|  0.0|(13424,[0,1,27,88...|[-1531.1264630880...|[0.04422736586211...|       1.0|\n",
      "|  0.0|(13424,[0,1,46,17...|[-1144.5161799816...|[2.56736897118619...|       1.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-95.709748556200...|[0.99999998670071...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3319.6602710011...|[1.0,7.4049278562...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,8,2...|[-1613.8828419202...|[1.0,1.0058574694...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2749.3960768063...|[1.0,9.3872021681...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2749.3960768063...|[1.0,9.3872021681...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-1619.7554050403...|[1.0,1.1484198788...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,1...|[-1310.6034902988...|[1.0,2.1045671841...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,25,...|[-419.92640892838...|[1.0,3.1261965383...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,8,1...|[-694.13925475605...|[1.0,2.0236257035...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-731.73354253890...|[1.0,7.5619763209...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,31,...|[-657.21436769793...|[1.0,1.1718462381...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,60,...|[-1357.0116142109...|[1.0,1.9461000496...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,14,...|[-475.16444599889...|[1.0,1.5921766538...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.931382333192883\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad considering we're using straight math on text data.  \n",
    "May come up with other engineering's features to get a better result."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
