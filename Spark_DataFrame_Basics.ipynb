{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(name='Basics').getOrCreate()  # name is just title to show on UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a *.json file (in the same folder of Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, \n",
    "                               StructType, \n",
    "                               IntegerType, \n",
    "                               StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a custom schema\n",
    "data_schema = [\n",
    "    StructField('age', IntegerType(), True), # field name: age, field type: Interger, Nullable: True\n",
    "    StructField('name', StringType(), True)  # filed name: name, field type: String, Nullable: True\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final structure\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file with custom schema\n",
    "df = spark.read.json('people.json', schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the custom schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('new_age', df['age']*3) # add new column name and its values\n",
    "                                           # No inplace option as in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|new_age|\n",
      "+----+-------+-------+\n",
      "|null|Michael|   null|\n",
      "|  30|   Andy|     90|\n",
      "|  19| Justin|     57|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change a column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "| age|   name|test|\n",
      "+----+-------+----+\n",
      "|null|Michael|null|\n",
      "|  30|   Andy|  90|\n",
      "|  19| Justin|  57|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed(existing='new_age', new='test') # Note: no inplace option as in pandas\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction with SQL: Spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates or replaces a local temporary view\n",
    "df.createOrReplaceTempView(name='people') # lifetime is tied to the :class:`SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query\n",
    "results = spark.sql('SELECT * FROM people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "| age|   name|test|\n",
      "+----+-------+----+\n",
      "|null|Michael|null|\n",
      "|  30|   Andy|  90|\n",
      "|  19| Justin|  57|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_age = spark.sql('SELECT * FROM people WHERE age=19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "|age|  name|test|\n",
      "+---+------+----+\n",
      "| 19|Justin|  57|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark dataframe basics operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a *.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('appl_stock.csv', \n",
    "                    inferSchema=True, # infers the input schema automatically from data\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date='2010-01-05', Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "|      Date|     Open|     High|      Low|    Close|  Volume|Adj Close|\n",
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "|2014-06-09|92.699997|93.879997|    91.75|93.699997|75415000|88.906324|\n",
      "|2014-06-10|94.730003|95.050003|    93.57|    94.25|62777000|89.428189|\n",
      "|2014-06-11|94.129997|94.760002|93.470001|93.860001|45681000|89.058142|\n",
      "|2014-06-12|94.040001|94.120003|91.900002|92.290001|54749000|87.568463|\n",
      "|2014-06-13|92.199997|92.440002|90.879997|91.279999|54525000|86.610132|\n",
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close < 100').show(5)  # just show 5 first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     Open|    Close|\n",
      "+---------+---------+\n",
      "|92.699997|93.699997|\n",
      "|94.730003|    94.25|\n",
      "|94.129997|93.860001|\n",
      "|94.040001|92.290001|\n",
      "|92.199997|91.279999|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close < 100').select(['Open', 'Close']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by dataframe operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "|      Date|     Open|     High|      Low|    Close|  Volume|Adj Close|\n",
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "|2014-06-09|92.699997|93.879997|    91.75|93.699997|75415000|88.906324|\n",
      "|2014-06-10|94.730003|95.050003|    93.57|    94.25|62777000|89.428189|\n",
      "|2014-06-11|94.129997|94.760002|93.470001|93.860001|45681000|89.058142|\n",
      "|2014-06-12|94.040001|94.120003|91.900002|92.290001|54749000|87.568463|\n",
      "|2014-06-13|92.199997|92.440002|90.879997|91.279999|54525000|86.610132|\n",
      "+----------+---------+---------+---------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     Open|    Close|\n",
      "+---------+---------+\n",
      "|92.699997|93.699997|\n",
      "|94.730003|    94.25|\n",
      "|94.129997|93.860001|\n",
      "|94.040001|92.290001|\n",
      "|92.199997|91.279999|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 100).select(['Open', 'Close']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by multiple conditions using operators: & (and), | (or), ~ (not), == (equal) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+---------+-----------------+---------+-----------------+\n",
      "|      Date|     Open|             High|      Low|            Close|   Volume|        Adj Close|\n",
      "+----------+---------+-----------------+---------+-----------------+---------+-----------------+\n",
      "|2014-06-16|91.510002|            92.75|91.449997|        92.199997| 35561000|        87.483064|\n",
      "|2014-06-20|91.849998|        92.550003|90.900002|        90.910004|100898000|        86.259066|\n",
      "|2014-06-23|    91.32|        91.620003|90.599998|90.83000200000001| 43694000|        86.183157|\n",
      "|2014-06-24|    90.75|        91.739998|90.190002|        90.279999| 39036000|        85.661292|\n",
      "|2014-06-25|90.209999|        90.699997|89.650002|        90.360001| 36869000|        85.737201|\n",
      "|2014-06-26|90.370003|        91.050003|89.800003|        90.900002| 32629000|        86.249576|\n",
      "|2014-06-27|    90.82|             92.0|90.769997|        91.980003| 64029000|        87.274325|\n",
      "|2016-05-13|     90.0|91.66999799999999|     90.0|        90.519997| 44392800|89.18571700000001|\n",
      "+----------+---------+-----------------+---------+-----------------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 100) & (df['Open'] < 92)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect: to get output as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df.filter((df['Close'] < 100) & (df['Open'] < 92)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2014-06-16', Open=91.510002, High=92.75, Low=91.449997, Close=92.199997, Volume=35561000, Adj Close=87.483064),\n",
       " Row(Date='2014-06-20', Open=91.849998, High=92.550003, Low=90.900002, Close=90.910004, Volume=100898000, Adj Close=86.259066),\n",
       " Row(Date='2014-06-23', Open=91.32, High=91.620003, Low=90.599998, Close=90.83000200000001, Volume=43694000, Adj Close=86.183157),\n",
       " Row(Date='2014-06-24', Open=90.75, High=91.739998, Low=90.190002, Close=90.279999, Volume=39036000, Adj Close=85.661292),\n",
       " Row(Date='2014-06-25', Open=90.209999, High=90.699997, Low=89.650002, Close=90.360001, Volume=36869000, Adj Close=85.737201),\n",
       " Row(Date='2014-06-26', Open=90.370003, High=91.050003, Low=89.800003, Close=90.900002, Volume=32629000, Adj Close=86.249576),\n",
       " Row(Date='2014-06-27', Open=90.82, High=92.0, Low=90.769997, Close=91.980003, Volume=64029000, Adj Close=87.274325),\n",
       " Row(Date='2016-05-13', Open=90.0, High=91.66999799999999, Low=90.0, Close=90.519997, Volume=44392800, Adj Close=89.18571700000001)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results  # a list of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### asDict() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2014-06-16', Open=91.510002, High=92.75, Low=91.449997, Close=92.199997, Volume=35561000, Adj Close=87.483064)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = results[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': '2014-06-16',\n",
       " 'Open': 91.510002,\n",
       " 'High': 92.75,\n",
       " 'Low': 91.449997,\n",
       " 'Close': 92.199997,\n",
       " 'Volume': 35561000,\n",
       " 'Adj Close': 87.483064}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35561000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Groupby and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sales_info.csv', inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Company').mean().show() # functions: mean(), max(), min(), sum(), count(), etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Sales': 'sum'}).show()  # Aggregation at column \"Sales\" with sum() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy and Aggregation together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_comp.agg({'Sales': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import functions from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (countDistinct, \n",
    "                                   avg, \n",
    "                                   stddev,\n",
    "                                   format_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Company)|\n",
      "+-----------------------+\n",
      "|                      4|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Company')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alias to change column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        Avg Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg('Sales').alias('Avg Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format_number to show short real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               std|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_std = df.select(stddev('Sales').alias('std'))\n",
    "df_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_std.select(format_number('std', 2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Sales', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('ContainsNull.csv', inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop: how option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop \n",
    "df.na.drop(how='any').show() # how='any': any null values\n",
    "                             # how='all': only drop if all null values in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop: threshold option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null values if in one row they ocurr in 2 columns (threshol=2)\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop: subset option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Sales']).show()  # drop null values in only a subset columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No name| null|\n",
      "|emp3|No name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_sales = [Row(avg(Sales)=400.5)]\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill with mean value\n",
    "from pyspark.sql.functions import mean\n",
    "mean_sales = df.select(mean(df['Sales'])).collect()\n",
    "print(f'mean_sales = {mean_sales}')  # [Row(avg(Sales)=400.5)]\n",
    "\n",
    "df.na.fill(mean_sales[0][0], subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Date and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth,\n",
    "                                   dayofweek,\n",
    "                                   dayofyear,\n",
    "                                   weekofyear,\n",
    "                                   hour,\n",
    "                                   month,\n",
    "                                   year,\n",
    "                                   format_number,\n",
    "                                   date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|2010|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a \"Year\" column\n",
    "new_df = df.withColumn('Year', year(df['Date']))\n",
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+------------------+-------------------+------------------+---------+\n",
      "|Year|         avg(Open)|         avg(High)|          avg(Low)|        avg(Close)|        avg(Volume)|    avg(Adj Close)|avg(Year)|\n",
      "+----+------------------+------------------+------------------+------------------+-------------------+------------------+---------+\n",
      "|2015|120.17575393253965|121.24452385714291| 118.8630954325397|120.03999980555547| 5.18378869047619E7|115.96740080555561|   2015.0|\n",
      "|2013| 473.1281355634922| 477.6389272301587|468.24710264682557| 472.6348802857143|         1.016087E8| 62.61798788492063|   2013.0|\n",
      "|2014| 295.1426195357143|297.56103184523823| 292.9949599801587| 295.4023416507935|6.315273055555555E7| 87.63583323809523|   2014.0|\n",
      "+----+------------------+------------------+------------------+------------------+-------------------+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy Year and calculate mean\n",
    "new_df.groupBy('Year').mean().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2015|120.03999980555547|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2012| 576.0497195640002|\n",
      "|2016|104.60400786904763|\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy Year and calculate mean then select only 2 columns\n",
    "result = new_df.groupBy('Year').mean().select(['Year', 'avg(Close)'])\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year| Avg Closing Price|\n",
      "+----+------------------+\n",
      "|2015|120.03999980555547|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2012| 576.0497195640002|\n",
      "|2016|104.60400786904763|\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.withColumnRenamed(existing='avg(Close)', new='Avg Closing Price')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Year|Avg Closing|\n",
      "+----+-----------+\n",
      "|2015|     120.04|\n",
      "|2013|     472.63|\n",
      "|2014|     295.40|\n",
      "|2012|     576.05|\n",
      "|2016|     104.60|\n",
      "|2010|     259.84|\n",
      "|2011|     364.00|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.select(['Year', format_number('Avg Closing Price', 2).alias('Avg Closing')])\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
